
                           PROYECTO - ANALISIS DE VIDEO
                                 Guillermo Monge

RESUMEN:

	El proyecto tiene dos objetivos principales. El primero consiste en video con el
	la detección de manera automática los cambios de escena (y diferenciarlos de los
	cambios de plano). El segundo objetivo es un tanto más ambicioso, detectar los
	distintos personajes que aparecen en el video y poder sacar datos de esto, como
	las escenas o tiempo que aparece un personaje en el video, o incluso la busqueda
	de escenas en las que un subconjunto de personajes coincidan.

GUION BASICO:

	1- Elaboración y manejo de herramientas básicas para el manejo de video.

	2- Estudio y definición de medidas sobre las imágenes que ayuden a determinar
		si dos frames pertenecen a la misma escena/plano o no.

	3- Desarrollo de una estructura de datos abstracta con la que vayamos almacenando
		datos sobre la escena/plano/personaje

	4- Aprendizaje de una mejor utilización de las herramientas de detección de caras
		y personas

	5- Aprendizaje de una mejor utilización de las herramientas de tracking de puntos
		críticos ( goodFeatures, SURF y FLANN )

	6- De manera equivalente a las escenas intentar definir un mismo tipo de medida
		que determine si dos caras son del mismo personaje o no.


DIARIO:

Semana 1- Durante los primeros días he empleado el tiempo en elaborar el guión básico
	del proyecto (expuesto arriba) y a la creación de unos programas básicos de 
	manejo de video (hello_video.py), el cual carga un video sobre el que podemos
	llevar varias acciones:
		-transformaciones de tamaño (pyrUp , pyrDown)
		-cambio de canal de color a blanco y negro y vice versa
		-captura y guardado de frames
		-aceleración y deceleración del framerate

	  Se me ocurrió que es posible que sea más fácil si me restrinjo a snippets de
	SitComs ya que tienen un número reducido de personajes principales, y sobre todo
	de escenarios fijos sobre los cuales se desarrollan las escenas. Es posible que
	para una primera aproximación también sea útil si tomo dibujos animados ya que
	las imágenes son mucho más controlables. He tomado un snippet de la serie 
	"Family Guy" sobre la cual empezar a trabajar.

	  Una vez con el programa de captura de frames, guardé diversos frames del
	snippet mencionado, y probé a ver la diferencia entre los frames por una
	simple resta de las imágenes (método que a partir de ahora denominaré como
	Raw Substraction o RS). El método no fue muy fructífero, por lo que pretendo
	la semana que viene trabajar con histogramas y la herramienta compareHistogram.

	  Encontré en YouTube dos videos interesantes con relación con el proyecto:

          -OpenCV Face Extraction / Detection + Processing ( http://tinyurl.com/cvw9zze )
		Detecta, extrae y guarda caras de la película Matrix Revolutions

	  -Predator: Camera That Learns ( http://tinyurl.com/3j463wp )
		Utiliza un sistema de machine learning muy parecido al que quiero
		implementar en la estructura de datos, sobre todo a la hora de
		definir la distancia entre caras de personajes distintos
		También enseña posibles aplicaciones muy interesantes como estabilizar
		una imagen

	  Empecé a organizar de manera esquemática la estructura de datos para las caras
	y escenas. Espero poder tenerla casi desarrollada entera para la tercera semana.



Semana 2- Ya que las pruebas con el RS de distintos frames los había llevado a cabo
	sobre la consola directamente, los plasmé en un pequeño programa a modo de
	ejemplo básico (frameDif.py). Posteriormente, en otro programa (rawSubsVideo.py)
	depuré la manera de trabajar con la misma técnica: el programa muestra el video
	y permite la captura de un frame. Cuando se captura un frame, también muestra
	el RS del frame capturado y el video. De esta manera, parece que si que se puede
	utilizar el método RS para una primera definición básica de distancia entre
	frames para la detección de cambio de escena. Aún así, sigue pareciendo mejor
	opción utilizar histogramas. En un intento de mejorar el manejo del programa,
	creé otro programa ( RSvideo-1wndw.py ) practicamente igual que el anterior, 
	pero con todos los vídeos e imágenes en la misma pantalla. Sin embargo, tarda
	demasiado tiempo en hacer la conversión y no queda bien.

	    NOTA - en el RS del frame con el video aparecen pequeños fallos, que muy
		posiblemente estén causados por ruido. Aplicar filtros para intentar
		reducirlo o eliminarlo.

	  Los intentos de  reducir el ruido que aparece al hacer la diferencia de
	frames no han sido muy fructíferos. Sigue apareciendo el ruido, aunque ahora
	mucho más difuminado que antes. Sin embargo, cuanto más blur-eada la imagen,
	los cambios de escena son más drásticos en el método RS. 

	  Los métodos utilizados para la reducción del sonido, se basaban en difuminar
	la imagen. En el programa ( rawSubsVideo.py ) se pueden cambiar los parámetros
	(y combinar) los dos principales:
		1 - Mediante cv2.blur (el resultado es muy parecido a si se hubiese
			aplicado un gaussianBlur o un medianBlur). No hay casi
			reducción en la velocidad.
		2 - Mediante pyrDown + pyrUp (repetido un número controlado de veces).
			Esta vez la reducción en la velocidad es considerable.

	  ( Histogram1.py ) Tras empezar a calcular los histogramas de cada frame
	capturado, con un simple vistazo se puede ver que esta técnica parece mucho
	más precisa a la hora de detectar cambios en la imagen, sobre todo cuando son
	cambios más bruscos (como de cambios de background, en vez de meros movimientos
	de los personajes).

	  La siguiente etapa es la del uso de las herramientas para comparar histogramas
	ver si estas son útiles para la detección de los tipos de cambios que queremos,
	y si no, intentar definir una distancia nueva.


	  Continué con la estructura de datos para las caras y escenas.

Semana 3- Por terminar de refinar la herramienta de análisis de los histogramas, en el
	programa Histogram2.py podemos observar el histograma del RGB en vez de en 
	escala de grises.

	  En la anterior reunión, Carlos y Alfonso me dieron una serie de ideas que
	durante la semana he ido probando:
		1 - Utilizar cv2.absdiff en vez de restarlo directamente. Sin embargo,
			el problema del ruido sigue apareciendo
		2 - Hacer muchos pyrDown's consecutivos para reducir la imagen a una
			matriz más simple. Esta si que da muy buenos resultados, se
			pueden observar en RS-pyrDown-separate.py y 
			RS-pyrDown-together.py , cuya diferencia es sobre que imagen
			hacen el pyrDown (together la hace al video nada más procesarlo,
			y separate toma la captura y luego hace pyrDown por separado a
			la caputra y al frame del video).
		3 - Considerar trabajar con el audio para el reconocimiento de personajes.
			Es una buena idea, y he estado investigando diversas bibliotecas
			para python, pero es posible que sea me lleve demasiado tiempo
			hasta que pueda manejarla bien como para meterlo en el proyecto.
			Las bibliotecas encontradas son:
				DragonFly - parece muy avanzada con muchas utilidades
				Audilab - parece más sencilla, no hace mucho más que
						convertir audio <-> numpy

	  También leí un poco a cerca de compareHist y substracción del background, pero
	por lo general la idea del pyrDown me parece buena y quiero seguir explorando
	por ese lado.

	  Ha sido una semana un poco menos productiva, pero la idea del pyrDown parece
	que pueda servir perfectamente para detectar cambios bruscos de background. 
	Espero que para la siguiente semana pueda tener ya algún programa que empiece
	a detectar de manera automática el cambio de plano/escena.

Semana 4- El objetivo principal es el de la detección automática del cambio de 
	escena/plano. La clave es como detectar que el frame de RS-substraction tiene
	mucho blanco.

	  Tras probar con diversos métodos para intentar la detección de cambio de
	escenas/planos el que mejores resultados da es una combinación. Consiste en 
	llevar a cabo bastantes pyrDown's (entre 4 y 6, según el tamaño inicial del video)
	restar el frame actual del anterior (amos reducidos) y calcular el histograma de
	la diferencia. Ese histograma de la diferencia (que no es más que el histograma de
	una imagen negra, con partes más blancas donde hay un cambio entre las imágenes)
	queremos poder analizarlo para poder detectar cuando "hay mucho blanco"(y por tanto
	hay una diferencia considerable entre las imágenes). Para ello llevamos a cabo un
	compareHistogram con el histograma de una imagen completamente negra. Cuanto mayor
	es la distancia entre los histogramas, mayor será la diferencia entre los frames.

	  Este proceso parece reducir los problemas que surgían al hacer hallar máximos en
	la diferencia (RS), o al comparar los dos histogramas directamente, ya que es un
	histograma más normalizado. Para conseguir el objetivo de la detección automática es
	necesario estudiar un poco las distintas distancias con las que se pueden comparar
	los histogramas: Correlation, Chi-Square, Intersect y Bhattacharyya.

	  Con el programa ( scene-hist.py ) se pueden observar las diferencias entre frames
	con cada una de las distancias. Utilizando este programa, analizando el snippet
	FamilyGuy.mp4 que utilizo como referencia base, he llegado a un par de thresholds.

	  En el programa ( detect_scene.py ) este método explicado anteriormente se lleva a
	cabo probando los thresholds mencionados. El que mejor funciona (con el video base, y 
	otros de Family Guy distintos) es la distancia Bhattacharyya.

	  El programa ( detect_scene.py ) procesa un video, y va analizando las diferencias
	entre frames. Cada vez que detecta un nuevo plano/escena, saca una nueva ventana con
	el primer frame del nuevo plano.

	  Tras unos pequeños cambios, el programa tiene bastante buena apariencia visual,
	saca las nuevas escena por filas y columnas distribuidas por toda la pantalla sin
	cubrir la venatana del video original o la de la anterior escena.

Semana 5- En la reunión descubrimos un pequeño fallo del programa con los trackings (o
	o movimientos de cámara), ya que los detectaba como nuevas escenas. El problema
	estaba en que el programa comparaba el nuevo frame con el primer frame de la anterior
	escena/plano. Comparando el frame actual, con el inmediatamente anterior se elimina
	este pequeño fallo y reduce a practicamente inexistente la cantidad de falsos
	positivos (que hasta el momento era muy bajo).

	  Durante la semana he ido probando el programa frente a distintos tipos de video.
	Los resultados han sido bastante sorprendentes, ya que el programa funciona casi a la
	perfección con el mismo threshold para la gran mayoría de los videos. Las únicas
	excepciones encontradas hasta el momento son los videos menos homogeneos. Por ejemplo,
	un partido de baloncesto. El problema no es tanto el cambio rápido de planos, o
	incluso planos muy largos, sino los cambios no controlados en la imagen (flashes de
	cámaras en el pabellón) ya que provocan cambios significantes en los datos de la imagen,
	pero no a la hora de nuestro ojo. Aun así, quitando estas ocasiones más concretas, el
	programa funciona muy bien.

	  En vistas a poder aplicarlo en la detección de caras y análisis de personajes en un
	video, he creado ( detect_compare.py ). El programa sigue el mismo procedimiento de
	proceso del video y detección de nuevos planos/escenas, sin embargo, cada vez que
	detecta una nueva escena la compara con todas los frames de escenas anteriores, y si
	se parece a alguna la considera una "copia" de la anterior, y en vez de aparecer como
	nueva escena aparece como copia de la anterior (a la misma altura que la ventana de la
	original). Es posible que esto no sea muy interesante a la hora de escenas, ya que los
	backgrounds se pueden repetir y queremos considerarlo escenas distintas, pero para la
	detección de caras/personajes este tipo de estructura va a ser fundamental.

	  Durante la semana, a Oleg le surgieron unos pequeños bugs a la hora del drawLine con
	su nueva versión de OpenCV 2.4, pero fueron resueltos sin mucho problema.

	  En la reunión se ha sugerido que podría ser interesante probar con el tracking de
	puntos característicos también. Pero es muy posible que esto baje mucho la eficiencia.

Semana 6- Con respecto a la eficiencia he añadido que salga la velocidad de procesamiento,
	fps, en la ventana del vieo original para analizar la rapidez. El programa
	( detect_compare.py ) con speed 1 (lo más rápido) procesa a unos 60~70 fps. Sin embargo,
	quitando el waitkey, cosa que a efectos práticos no es necesario tanto el pausar el vídeo
	o poder modificar la velocidad, llega a procesar a unos 300 fps. De hecho, en el portatil
	de Oleg, el video llegó a procesarse antes de poder cargar todas las imágenes. Esto son
	muy buenas noticias, ya que es muy interesante poder procesar una película correctamente
	a alta velocidad, así una película de unas dos horas podría ser procesada en 10~15
	minutos.

	  El siguiente paso es empezar con la deteccíon de caras, pero para ello está pendiente
	que el pequeño script de reconocer caras de Syrus funciones correctamente.


	  





