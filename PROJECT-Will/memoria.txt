
                           PROYECTO - ANALISIS DE VIDEO
                                 Guillermo Monge

RESUMEN:

	El proyecto tiene dos objetivos principales. El primero consiste en video con el
	la detección de manera automática los cambios de escena (y diferenciarlos de los
	cambios de plano). El segundo objetivo es un tanto más ambicioso, detectar los
	distintos personajes que aparecen en el video y poder sacar datos de esto, como
	las escenas o tiempo que aparece un personaje en el video, o incluso la busqueda
	de escenas en las que un subconjunto de personajes coincidan.

GUION BASICO:

	1- Elaboración y manejo de herramientas básicas para el manejo de video.

	2- Estudio y definición de medidas sobre las imágenes que ayuden a determinar
		si dos frames pertenecen a la misma escena/plano o no.

	3- Desarrollo de una estructura de datos abstracta con la que vayamos almacenando
		datos sobre la escena/plano/personaje

	4- Aprendizaje de una mejor utilización de las herramientas de detección de caras
		y personas

	5- Aprendizaje de una mejor utilización de las herramientas de tracking de puntos
		críticos ( goodFeatures, SURF y FLANN )

	6- De manera equivalente a las escenas intentar definir un mismo tipo de medida
		que determine si dos caras son del mismo personaje o no.


DIARIO:

Semana 1- Durante los primeros días he empleado el tiempo en elaborar el guión básico
	del proyecto (expuesto arriba) y a la creación de unos programas básicos de 
	manejo de video (hello_video.py), el cual carga un video sobre el que podemos
	llevar varias acciones:
		-transformaciones de tamaño (pyrUp , pyrDown)
		-cambio de canal de color a blanco y negro y vice versa
		-captura y guardado de frames
		-aceleración y deceleración del framerate

	  Se me ocurrió que es posible que sea más fácil si me restrinjo a snippets de
	SitComs ya que tienen un número reducido de personajes principales, y sobre todo
	de escenarios fijos sobre los cuales se desarrollan las escenas. Es posible que
	para una primera aproximación también sea útil si tomo dibujos animados ya que
	las imágenes son mucho más controlables. He tomado un snippet de la serie 
	"Family Guy" sobre la cual empezar a trabajar.

	  Una vez con el programa de captura de frames, guardé diversos frames del
	snippet mencionado, y probé a ver la diferencia entre los frames por una
	simple resta de las imágenes (método que a partir de ahora denominaré como
	Raw Substraction o RS). El método no fue muy fructífero, por lo que pretendo
	la semana que viene trabajar con histogramas y la herramienta compareHistogram.

	  Encontré en YouTube dos videos interesantes con relación con el proyecto:

          -OpenCV Face Extraction / Detection + Processing ( http://tinyurl.com/cvw9zze )
		Detecta, extrae y guarda caras de la película Matrix Revolutions

	  -Predator: Camera That Learns ( http://tinyurl.com/3j463wp )
		Utiliza un sistema de machine learning muy parecido al que quiero
		implementar en la estructura de datos, sobre todo a la hora de
		definir la distancia entre caras de personajes distintos
		También enseña posibles aplicaciones muy interesantes como estabilizar
		una imagen

	  Empecé a organizar de manera esquemática la estructura de datos para las caras
	y escenas. Espero poder tenerla casi desarrollada entera para la tercera semana.



Semana 2- Ya que las pruebas con el RS de distintos frames los había llevado a cabo
	sobre la consola directamente, los plasmé en un pequeño programa a modo de
	ejemplo básico (frameDif.py). Posteriormente, en otro programa (rawSubsVideo.py)
	depuré la manera de trabajar con la misma técnica: el programa muestra el video
	y permite la captura de un frame. Cuando se captura un frame, también muestra
	el RS del frame capturado y el video. De esta manera, parece que si que se puede
	utilizar el método RS para una primera definición básica de distancia entre
	frames para la detección de cambio de escena. Aún así, sigue pareciendo mejor
	opción utilizar histogramas. En un intento de mejorar el manejo del programa,
	creé otro programa ( RSvideo-1wndw.py ) practicamente igual que el anterior, 
	pero con todos los vídeos e imágenes en la misma pantalla. Sin embargo, tarda
	demasiado tiempo en hacer la conversión y no queda bien.

	    NOTA - en el RS del frame con el video aparecen pequeños fallos, que muy
		posiblemente estén causados por ruido. Aplicar filtros para intentar
		reducirlo o eliminarlo.

	  Los intentos de  reducir el ruido que aparece al hacer la diferencia de
	frames no han sido muy fructíferos. Sigue apareciendo el ruido, aunque ahora
	mucho más difuminado que antes. Sin embargo, cuanto más blur-eada la imagen,
	los cambios de escena son más drásticos en el método RS. 

	  Los métodos utilizados para la reducción del sonido, se basaban en difuminar
	la imagen. En el programa ( rawSubsVideo.py ) se pueden cambiar los parámetros
	(y combinar) los dos principales:
		1 - Mediante cv2.blur (el resultado es muy parecido a si se hubiese
			aplicado un gaussianBlur o un medianBlur). No hay casi
			reducción en la velocidad.
		2 - Mediante pyrDown + pyrUp (repetido un número controlado de veces).
			Esta vez la reducción en la velocidad es considerable.

	  ( Histogram1.py ) Tras empezar a calcular los histogramas de cada frame
	capturado, con un simple vistazo se puede ver que esta técnica parece mucho
	más precisa a la hora de detectar cambios en la imagen, sobre todo cuando son
	cambios más bruscos (como de cambios de background, en vez de meros movimientos
	de los personajes).

	  La siguiente etapa es la del uso de las herramientas para comparar histogramas
	ver si estas son útiles para la detección de los tipos de cambios que queremos,
	y si no, intentar definir una distancia nueva.


	  Continué con la estructura de datos para las caras y escenas.

Semana 3- Por terminar de refinar la herramienta de análisis de los histogramas, en el
	programa Histogram2.py podemos observar el histograma del RGB en vez de en 
	escala de grises.

	  En la anterior reunión, Carlos y Alfonso me dieron una serie de ideas que
	durante la semana he ido probando:
		1 - Utilizar cv2.absdiff en vez de restarlo directamente. Sin embargo,
			el problema del ruido sigue apareciendo
		2 - Hacer muchos pyrDown's consecutivos para reducir la imagen a una
			matriz más simple. Esta si que da muy buenos resultados, se
			pueden observar en RS-pyrDown-separate.py y 
			RS-pyrDown-together.py , cuya diferencia es sobre que imagen
			hacen el pyrDown (together la hace al video nada más procesarlo,
			y separate toma la captura y luego hace pyrDown por separado a
			la caputra y al frame del video).
		3 - Considerar trabajar con el audio para el reconocimiento de personajes.
			Es una buena idea, y he estado investigando diversas bibliotecas
			para python, pero es posible que sea me lleve demasiado tiempo
			hasta que pueda manejarla bien como para meterlo en el proyecto.
			Las bibliotecas encontradas son:
				DragonFly - parece muy avanzada con muchas utilidades
				Audilab - parece más sencilla, no hace mucho más que
						convertir audio <-> numpy

	  También leí un poco a cerca de compareHist y substracción del background, pero
	por lo general la idea del pyrDown me parece buena y quiero seguir explorando
	por ese lado.

	  Ha sido una semana un poco menos productiva, pero la idea del pyrDown parece
	que pueda servir perfectamente para detectar cambios bruscos de background. 
	Espero que para la siguiente semana pueda tener ya algún programa que empiece
	a detectar de manera automática el cambio de plano/escena.

Semana 4- El objetivo principal es el de la detección automática del cambio de 
	escena/plano. La clave es como detectar que el frame de RS-substraction tiene
	mucho blanco.


	  





