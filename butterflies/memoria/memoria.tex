\documentclass[12pt,a4paper,spanish]{report}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{amssymb, latexsym}
\usepackage[spanish]{babel}
%\selectlanguage{spanish}  
\usepackage[latin1]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{tocloft}
\usepackage{color}

\lstset{breaklines=true, breakatwhitespace=false}
\lstset{numbers=left, numberstyle=\scriptsize}

\newcommand{\notaCarlos}[1]{{\color{red}#1}}

%\setlength\cftparskip{-2pt}
%\setlength\cftbeforechapskip{0pt}

\hoffset=-0.5cm
\voffset=-1.5cm
\textwidth=15cm
\textheight=22cm

\setlength{\parskip}{\baselineskip}

\titleformat{\chapter}
{\Huge\bfseries}{\thechapter . }{0pt}
{}
\titlespacing{\chapter}{0pt}{-20pt}{20pt}


\begin{document}

\begin{titlepage}
\begin{center}
\vspace*{0in}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=15cm,height=10cm]{imagenes/portada4.jpg}
\end{center}
\end{figure}
\large{UNIVERSIDAD COMPLUTENSE DE MADRID}\\
\vspace*{0.07in}
\normalsize{FACULTAD DE CC.MATEMÁTICAS}\\
\vspace*{0.3in}
\begin{LARGE}
\textbf{VISIÓN COMPUTARIZADA APLICADA A LA BIOLOGÍA} \\
\end{LARGE}
\vspace*{0.3in}
\rule{80mm}{0.1mm}\\
\vspace*{0.1in}
\begin{large}
Autora: Gema Valdés Berlinches \\
Supervisado por: Carlos Gregorio Rodríguez \\
\end{large}
\end{center}
\end{titlepage}


\newpage
\tableofcontents

\newpage
\chapter{Introducción}
Esta memoria muestra el camino que hemos seguido para realizar un Trabajo Académicamente Dirigido (TAD). En un principio el trabajo iba a ser sobre visión computarizada con aplicaciones en la medicina e iba a constar de dos partes, una primera parte de estudio de la bibliografía para adquirir unos conocimientos básicos sobre las técnicas de computer vision y una segunda parte para desarrollar un problema con aplicaciones en medicina. Al finalizar la primera parte y debido a que el material que necesitábamos para empezar con la investigación del problema no estaba disponible, nos vimos en la obligación de cambiar a un problema con aplicaciones en biología, mas en concreto, sobre mariposas.  

La visión computarizada o también conocida como visión artificial es un nuevo campo de investigación cuyo objetivo principal es que el ordenador entienda lo que ve y que está muy relacionada con todas las ramas de la matemática, la estadística, el análisis, el álgebra,... A lo largo del documento se pueden observar los conceptos matemáticos que se encuentran tras algunos de los métodos y que van desde cosas tan sencillas como el cálculo de un gradiente o una función a cosas más complejas tras las que acaba apareciendo el teorema de Green o la transformada de Fourier.\\
Esta nueva disciplina tiene muchos posibles campos de aplicación como biología, medicina, video-juegos, robótica, etc. y por tanto muchos nuevos desafíos y problemas por resolver.

Para mí era un campo totalmente desconocido que de primeras me causó mucho respeto y me parecía algo casi inalcanzable pero que a su vez me llamaba mucho la atención y que a medida que me adentraba en el tema me apetecía seguir aprendiendo más. Finalmente me ha resultado una disciplina muy atractiva e interesante, tanto que próximamente, continuaré estudiando sobre el tema realizando un máster sobre visión artificial.

Debido a que esta carrera, matemáticas, es muy teórica, había algunos momentos en que se me olvida que relación tenía lo que estaba estudiando con la realidad, llegando en algunos momentos a pensar: \textit{¿que hago yo aquí?}. Por eso yo, una persona que no tenía un perfil definido, encontré en la computación una salida, donde veía, rápidamente el fruto y las aplicaciones de lo que estudiaba y donde encontraba un sentido a lo que aprendía en muchas asignaturas.\\
Desde mi punto de vista, durante los primeros cursos deberían estar más presentes los motivos por los cuales se estudia cada asignatura y relacionarse más los contenidos de estas con los de otras asignaturas y, en definitiva, con la vida real. Esto es algo que yo he echado en falta en algunos momentos a lo largo de mi camino (aunque no se si será así siempre o a sido sólo mi caso). Me parece que se podía animar más a la gente de los primeros cursos poniendo a su disposición por ejemplo, los trabajos realizados en TAD's para que no se desanime y vean las cosas que se pueden llegar a hacer gracias a los conocimientos que se adquieren.\\
Creo que este sería el momento de recordar a aquellos buenos profesores que, de vez en cuando, aparecían y te animaban y daban motivos para seguir adelante y no abandonar.

Por último falta hablar de los objetivos. El objetivo principal era conocer el campo de la visión artificial, más concretamente, trabajar con técnicas y procedimientos básicos sobre procesamiento de imágenes para poder plantear y resolver problemas sobre visión computarizada y manejar las herramientas que nos proporcionan algunas librerías como OpenCV. A su vez, también formaba parte de los objetivos el ser capaz de plasmar todas las investigaciones y el trabajo realizado en el presente documento.\\
El objetivo final del trabajo era, partiendo de cero llegar a adquirir los conocimientos y soltura necesarios para encontrar una solución eficaz al problema concreto que se plantea en el siguiente capítulo.


\chapter{Presentación y objetivos del proyecto}

\subsection*{Problema}
Dada una base de datos con imágenes de mariposas deseamos encontrar un método que nos facilite la tarea de ver si una nueva mariposa la tenemos ya clasificada o, por el contrario, todavía no se encuentra dentro de la colección. Para ello, necesitamos discrimar las imágenes de diferentes formas y así seleccionar las más parecidas a la nueva, para que de esta manera, sea más fácil tomar la decisión de si está clasificada o no, ya que tendremos que fijarnos en un número muy reducido de ejemplares en vez de tener que mirar todas una por una.
 
\subsection*{Objetivos}
El objetivo del trabajo es crear un programa que, utilizando la imagen de la nueva mariposa, vaya comparando ésta con cada una de las que tenemos ya guardadas en nuestra base de datos y que decida, según diferentes criterios, si son suficientemente parecidas y es seleccionada, o no lo son y es descartada, obteniendo como resultado el conjunto de todas las seleccionadas, es decir, las más semejantes a la que deseamos añadir.

Posteriormente, tras realizar un estudio de las muestras se decide que, realizar las comparaciones utilizando la información sobre el color de los ejemplares y algunas características sobre la forma, sería lo más adecuado.

\subsection*{Muestras}
Las imágenes que vamos a utilizar en el trabajo están sacadas de la página web de Proyecto Mariposa \url{proyectomariposa.net}, que es un proyecto cuyo objetivo es crear una base de datos de la biodiversidad de las mariposas diurnas de Colombia y testar la utilidad del código de barras genético. Estas muestras ya estaban tomadas antes de comenzar nuestro estudio y están pensadas para el tratamiento humano y no automático, por ello, para facilitar este cambio, de todas las imágenes que están disponibles vamos a trabajar sólo con las que tienen debajo un \textit{color-check} (véase \textit{Figura \ref{qpcard}}).\\
Las muestras tienen, por tanto, un ejemplar de mariposa en la parte superior que está encuadrado por dos reglas, una a la izquierda y otra debajo, que nos van a indicar el tamaño y debajo de la regla inferior un \textit{color-check}(véase \textit{Figura \ref{ejemplar}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/qp.jpg}
\end{center}
\caption{Color-check.}
\label{qpcard}
\end{figure}\begin{figure}[ht]

\begin{center}
\includegraphics[width=10cm]{imagenes/ejemplar.jpg}
\end{center}
\caption{Imagen de una de las muestras.}
\label{ejemplar}
\end{figure}


\subsection*{Solución} 

Los principales pasos que hay que llevar a cabo son:
\begin{itemize}
\item Hacer un preprocesamiento de las imágenes, haciendo que todas estén a la misma escala, para que los resultados de los análisis posteriores sean más reales.
\item Encontrar la silueta de la mariposa para poder aislarla del fondo, llegando a obtener una 'máscara', en la que los puntos del cuerpo de la mariposa son blancos y el resto negros. 
\item Hacer las comparaciones utilizando información sobre el color y algunas propiedades características de la forma. Para que los datos que analizamos sean sólo de la mariposa utilizaremos la máscara.
\end{itemize}  

\subsection*{Estructura}
En el documento se pueden encontrar una parte principal donde se explica el desarrollo del proyecto y varios apéndices.

La parte principal consta de varios capítulos:
 \begin{itemize}
  \item Capítulo 3: Primera toma de contacto con las muestras y preprocesamiento previo. Se explican los pasos seguidos para encontrar el color-check y como se hace el reescalado utilizando una regla de tres.
  \item Capítulo 4: Se trata de aislar la mariposa para obtener una máscara y realizar las posteriores comparaciones fijándonos, únicamente en propiedades del ejemplar. Es el núcleo principal del trabajo. Se comentan algunos problemas encontrados al empezar a trabajar con las muestras, como las sombras o la diversidad de colores. Algunos caminos que fueron investigados y luego no dieron fruto. Por último, los pasos seguidos hasta llegar a la solución final: eliminar ruido, acentuar los bordes, separar el fondo, etc.
  \item Capítulo 5: Se llevan a cabo las comparaciones basándonos en distintas propiedades del color y la forma. Para decidir la similitud entre muestras utilizando la información sobre el color se utilizarán histogramas. Para comparar los ejemplares fijándonos en la forma se van a utilizar distintas propiedades extraídas de la máscara o los contornos. De la máscara utilizaremos unos valores (momentos) que son invariantes a escalado, traslación,... Con los contornos se aplicarán algunas funciones propias de la librería. Por último, también se utilizarán como valores para la comparación algunas propiedades como el área o el perímetro del contorno, la proporción entre alto y ancho del rectángulo de mínimo área que contiene al contorno, etc. 
  \item Capítulo 6: Aplicación final e interfaz. Se comenta como juntando los distintos programas que teníamos se consigue la aplicación final y se explican las distintas opciones disponibles en el interfaz.
  \item Capítulo 7: Se comentan algunas posibles mejoras en la toma de muestras y las conclusiones.  
 \end{itemize}

También hay tres apéndices.
\begin{itemize}
  \item Apéndice A: Se explican los principales conceptos sobre el tratamiento de imágenes que son necesarios para entender y poder seguir el documento. A lo largo del texto se hacen múltiples referencias a este apéndice para que se pueda entender el funcionamiento de los métodos que se utilizan.\\
 Se divide principalmente en tres partes: 
    \begin{enumerate}
      \item Filtrado de imágenes.
      \item Análisis de imágenes.
      \item Contornos.
    \end {enumerate}
  \item Apéndice B: El resultado final del trabajo es una aplicación escrita en el lenguaje de programación Python y por ese motivo en este apéndice se puede encontrar parte del código desarrollado. 
  \item Apéndice C: Se muestran varias pruebas del funcionamiento del programa final. Debido a la gran cantidad de imágenes, en estos ejemplos no se utilizan todos los ejemplares disponibles, sino una pequeña muestra.\\
Se pueden ver varios ejemplos de ejecución de los distintos programas de búsqueda por separado y uno de la aplicación final. En cada prueba se hace un pequeño comentario sobre los resultados obtenidos. 
 \end{itemize}


 \subsection*{Nomenclatura y notación}
Antes de continuar vamos a explicar la notación que va a ser utilizada.

\begin{itemize}
\item Vamos a nombrar muchas funciones de la librería OpenCV. Esta es una librería de visión computarizada destinada al tratamiento de imágenes y visión por ordenador en tiempo real (ver \ref{opencv}).\\
 Estas funciones ya están implementadas. Para nombrarlas utilizaremos la letra  \texttt{typewriter}, por ejemplo, \texttt{cv2.Canny}.\\
 Algunas de ellas son muy importantes y se utilizan muchas veces a lo largo del proyecto por ello es importante entender bien su funcionamiento. De éstas se pueden encontrar explicaciones más detalladas y algunos ejemplos de utilización en el apéndice A. Tras nombrarlas indicaremos en que parte del apéndice se pueden encontrar.
\item También nombraremos funciones propias que se han ido implementando a medida que se desarrollaba el proyecto, las nombraremos también con letra \texttt{typewriter}, por ejemplo, \texttt{flodfillgray.py}, ya que están implementadas en python (utilizando la librería OpenCV).\\
 Como muchas de ellas se utilizan posteriormente en otros programas y en algunas se han seguido muchos pasos hasta la obtención de la solución final, para poder entender mejor su funcionamiento se puede encontrar el código en el apéndice B.\\
\item Por último decir que en algunos puntos hay referencias a páginas web donde se puede ampliar la información. 
\end{itemize}

\chapter{Preprocesando las muestras}
Las imágenes obtenidas de la base de datos de Proyecto Mariposa están a distinta escala, por tanto, es necesario llevar a cabo un proceso previo de reescalado para hacer que todas queden igual y no obtener datos falsos en las posteriores comparaciones.\\
Hacer este reescalado consiste en encontrar el color-check en cada imagen y hacer que éste tenga el mismo tamaño en cada una de ellas (véase \textit{Figura \ref{igual-qp}}), así, 1cm de los que marcan las reglas de las imágenes va a equivaler al mismo número de píxeles en cada una de ellas.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/Imagen1.jpg}
\end{center}
\caption{Todas las \textit{color-check} tienen el mismo tamaño.}
\label{igual-qp}
\end{figure}


\section{Encontrando el \textit{color-check}}
Vamos a intentar encontrar la posición del color-check encontrando su contorno y para que esto sea más sencillo, hay que lograr que destaque sobre el fondo. 

Empezamos probando con diferentes thresholds aplicados sobre toda la imagen (ver \ref{section theshold}). Utilizando los programas \texttt{test\_threshold.py} y \texttt{adapThres.py} con varias imágenes distintas se comprueba que el threshold adaptativo podría funcionar bien, pero al empezar a probar, rápidamente se observa que no va a ser así ya que, independientemente de los parámetros que se tomen, algunos de los colores se quedan en blanco y en varias imágenes llegan a hacer que el rectángulo quede dividido en dos partes (véase \textit{Figura \ref{adapThresh}}).

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{imagenes/Imagen2.png}
\end{center}
\caption{Fallos en el threshold adaptativo.}
\label{adapThresh}
\end{figure} 

Como aplicar un threshold directamente parece que no funciona bien, guardamos una imagen del color-check aislado (qp.jpg) y probamos con \texttt{cv2.matchTemplate}. 

Lo que hace este método es, dado un patrón de tamaño $h\times w$, va desplazándolo sobre la imagen de tamaño $H\times W$ y comparándolos según hayamos elegido. Si por ejemplo se utiliza el método del cuadrado de las diferencias normalizado, lo que se obtiene es una nueva imagen de tamaño $(W-w+1)\times (H-h+1)$ donde cada punto representa lo parecido que es el patrón a la porción de la imagen con que lo estamos comparando, siendo 0 si son perfectamente iguales y mayor cuanto más distintas. El punto donde se encuentra el mínimo en esta nueva imagen es el valor de la esquina superior izquierda de la porción de imagen donde es más probable que se encuentre el patrón.

Aplicándolo sobre nuestras imágenes tomando como patrón el color-check, se observa que funciona muy bien, pero como los tamaños de éstos varían de unas imágenes a otras, no se obtiene el punto exacto donde se encuentra en la mayoría de ellas. Finalmente decidimos aprovechar este punto para reducir el área de la imagen donde buscar, quedándonos con la porción situada desde el punto hacia la derecha y hacia abajo, añádiendo un trozo por arriba y hacia la izquierda lo suficientemente grande para asegurarnos que el color-check se encuentra en esta zona.

Como los filtros que estábamos probando no funcionaban muy bien, creamos \texttt{floodfillgray.py} (ver \ref{prog floodfillgray}) que actúa sobre los puntos con un valor de r, g y b parecido (grises), evitando los blancos y negros (ver \ref{section-floodfillgray}). Lo utilizamos para acentuar las diferencias entre fondo y color-check, ya que el fondo tiene un color gris neutro (véase \textit{Figura \ref{floodfillgray}}). Este filtro sí que funciona como deseábamos, acentuando el color-chechk sobre el fondo, y además sirve para todas las imágenes, así que es el que utilizamos.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/ffgray.jpg}
\end{center}
\caption{Ejemplo de funcionamiento de \texttt{floodfillgray.py}}
\label{floodfillgray}
\end{figure} 

Una vez que conseguimos separar la tarjeta de color del fondo, el siguiente paso era encontrar el contorno. Para ello utilizamos \texttt{cv2.Canny} (ver \ref{section canny})seguido de \texttt{cv2.findContours}. Como el color-check es un rectángulo, lo que hacemos para estar seguros de que el contorno seleccionado es el correcto es calcular el rectángulo de área mínima que contiene a cada uno (\texttt{cv2.boundingRect}) y quedarnos con el que tenga el área y la proporción alto/ancho más parecido al del color-check aislado que habíamos guardado.

En resumen, los pasos a seguir son:
\begin{itemize}
\item Primero aplicar \texttt{cv2.matchTemplate} utilizando como patrón la imagen del color-check aislada.
\item Después aplicar nuestro filtro \texttt{floodfillgray.py} para destacar el color-check sobre el fondo.
\item Seguidamente \texttt{cv2.Canny} para encontrar todos los contornos.
\item Finalmente, para elegir el contorno adecuado, usamos \texttt{cv2.boundingRect} sobre cada contorno y seleccionamos el más parecido al color-check.
\end{itemize}

\section{Reescalando la imagen}
Una vez encontrados los color-check en cada imagen hay que llevar a cabo el reescalado. Esto consiste en hacer que todos ellos tengan el mismo tamaño utilizando una simple regla de tres. Para intentar perder la menor información posible sobre las imágenes, principalmente del color, el nuevo valor de alto y ancho  elegido es la media de los altos y anchos de cada imagen.

Uniendo todo, se obtiene el programa \texttt{resize\_and\_write.py} (ver \ref{prog resizeAndWrite}) que hace lo que deseábamos: 
\begin{itemize}
\item Encontrar en cada imagen el color-check.
\item Utilizando éste, hacer que todas las imágenes queden a la misma escala.
\end{itemize}  

\chapter{Aislando la mariposa}
Ya que habíamos conseguido tener todas las imágenes guardadas a la misma escala había que encontrar una máscara o contorno de la mariposa para tenerla aislada, es decir, sin influencia de los valores del fondo ni de ningún otro elemento sobre los datos que se obtuvieran.

Esta parte del trabajo aparentemente era muy sencilla ya que a simple vista parecía que aplicando un threshold o \texttt{cv2.floodfill} sería suficiente para lograr lo que deseábamos, pero cuando nos pusimos a trabajar en ello nos dimos cuenta de que no es así. Al contrario de lo que creíamos, debido a problemas con las muestras y a la gran variedad de ejemplares (algunos muy diferentes), aislar la mariposa se convirtió en un gran problema y, por tanto, en el núcleo del trabajo.  

\section{Problemas de las muestras}
Al empezar a trabajar sobre las muestras surgieron dos grandes problemas: las sombras en la zona inferior de las mariposas y la gran diversidad de colores (véase \textit{Figura \ref{sombras}} y \textit{Figura \ref{diversoscolores}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/sombras.jpg}
\end{center}
\caption{Muestra con sombras.}
\label{sombras}
\end{figure} 

Las sombras fueron un problema debido a que, al aplicar algunos métodos como \texttt{cv2.Canny} para encontrar el borde, detectaba las sombras como contorno y al aplicar otros como \texttt{cv2.floodFill}, hacían de ``barrera'' impidiendo que esa zona del fondo fuera cubierta o que si aumentabas los límites para cubrirla se tapase parte de la mariposa. Aquí también entraba en juego el problema de los colores, ya que si no hubiera muestras con colores claros, las sombras realmente no serían tan problemáticas, porque con disminuir el valor del límite inferior en \texttt{cv2.floodFill} conseguiríamos taparlas sin entrar en el ``cuerpo'' (esto es así debido a la gran diferencia entre los valores del color del fondo y los de la mariposa). Pero como los ejemplares tienen colores muy variados en vez de evitar un problema nos encontramos con dos.

El que los ejemplares de las muestras tuvieran colores muy variados, aparte de no ayudar a la hora de quitar las sombras también era un problema porque metódos que funcionaban bien con unos tonos no funcionaban tan bien con otros (un ejemplo de ésto puede ser el ya mencionado \texttt{cv2.floodFill}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/diversoscolores.jpg}
\end{center}
\caption{Muestra con muchos colores}
\label{diversoscolores}
\end{figure} 

Otra de las dificultades que nos encontramos fue que las mariposas podían tener muchos colores. Debido a ésto, no pudimos utilizar directamente métodos como \texttt{cv2.threshold} ya que si, por ejemplo, el ejemplar era marrón con alguna mancha blanca, a parte de tapar el fondo taparía también las manchas.

\section{Solución obtenida}
Después de probar muchos métodos con muchos valores diferentes y de investigar varios caminos que resultaron no ser útiles (véase sección \ref{caminos no fructiferos}) llegamos a una posible solución.

Antes de seguir y para que tardara menos en hacer los cálculos decidimos reducir el tamaño de la imagen con la que trabajar. Lo que se hace es utilizar \textit{cv2.matchTemplate} para encontrar la esquina superior izquierda del color-check, a este punto se le resta una constante $a$ de forma que se ajuste lo máximo posible a la mariposa (véase \textit{Figura \ref{ajusteCte}}) (esta constante $a$ es conocida ya que es el valor correspondiente a la parte que todavia quedaría visible de la regla, y esto es posible debido a que la escala a la que está la imagen es conocida).

\begin{figure}[h]
\begin{center}
\includegraphics[width=8cm]{imagenes/Imagen4.png}
\end{center}
\caption{Valor que hay que restar.}
\label{ajusteCte}
\end{figure} 

Una vez que teníamos reducida la imagen empezamos a aplicar distintos filtros, con el fin de llegar a obtener una máscara de la mariposa o bien una imagen en la que destacara ésta sobre el fondo de forma que fuera fácil encontrar el contorno.


\subsection{Acentuar contorno}
Una primera idea fue usar \textit{cv2.floodFill} (ver \ref{section floodfill}). Esto funcionaba muy bien sobre mariposas con colores oscuros, pero las que tenían colores claros nos daban problemas, ya que si tomábamos valores bajos en el límite inferior no inundaba bien todo el fondo y si aumentábamos estos valores se inundaba también parte de la mariposa (véase \textit{Figura \ref{claro-oscuro}}). 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=15cm]{imagenes/Imagen5.png}
\end{center}
\caption{La mariposa de colores oscuros se aisla muy facilmente, sin embargo, la de colores claros se inunda rápidamente.}
\label{claro-oscuro}
\end{figure}

Como aplicar directamente \texttt{cv2.floodFill} no era suficiente decidimos probar acentuando antes el contorno. Aplicamos \texttt{cv2.medianBlur} (ver \ref{section blur}) y \texttt{cv2.erode} (ver \ref{section erode dilate}) para acentuar los bordes de la mariposa (véase \textit{Figura \ref{blur-erode}}). Esto funcionaba muy bien. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{imagenes/Imagen6.png}
\end{center}
\caption{Transformación de la imagen tras aplicar \texttt{cv2.medianBlur} y \texttt{cv2.erode}.}
\label{blur-erode}
\end{figure}

Después de ésto, aplicamos otra vez \texttt{cv2.Canny} para encontrar los contornos, pero viendo las imágenes que generaba, observamos que acentuar sólo los bordes no era suficiente para encontrar bien el contorno (véase \textit{Figura \ref{canny}}), pero sí que podíamos aprovechar los contornos obtenidos como máscara con \texttt{cv2.floodFill}. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/canny.jpg}
\end{center}
\caption{Aplicar \texttt{cv2.Canny} no es suficiente para encontrar el contorno pero se puede usar como máscara.}
\label{canny}
\end{figure}

Gracias al uso conjunto de \texttt{cv2.floodFill} y \texttt{cv2.Canny} logramos que en las imágenes con colores claros, cuando aplicábamos \texttt{cv2.floodFill}, fuera más difícil tapar parte de la mariposa ya que no podía atravesar los contornos que utilizamos como máscara (ver \ref{section floodfill}).

Por tanto, los pasos que finalmene seguimos son:
\begin{itemize}
\item Acentuar el contorno utilizando \texttt{cv2.medianBlur} y \texttt{cv2.erode}.
\item Aplicar \texttt{cv2.Canny} para encontrar los contornos que utilizaremos como máscara.
\item Por último, aplicar el método \texttt{cv2.floodFill} en conjunto con la máscara.
\end{itemize}

\subsection{Separar el fondo}
Finalmente, para separar el fondo aplicamos varias veces seguidas el método \texttt{cv2.floodFill} utilizando la imagen de canny como máscara (véase \textit{Figura \ref{puntosff}}).\\
Los puntos sobre los que utilizamos este método fueron, en principio, la esquina superior derecha de la imagen, para así asegurarnos de que el píxel elegido no estba dentro del cuerpo de la mariposa y, como con un único punto no iba a ser suficiente para cubrir todo el fondo, ya que por la zona de las reglas el color es más oscuro por las sombras, tomamos varios puntos por el borde izquierdo y el inferior de la imagen que son donde surge el problema.\\
Además de sobre estos puntos el usuario puede aplicar el método tantas veces como desee para lograr que la máscara se adapte al cuerpo de la mariposa lo máximo posible.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/puntosff.jpg}
\end{center}
\caption{\texttt{cv2.floodFill} varias veces con imagen de \texttt{cv2.Canny} como máscara.}
\label{puntosff}
\end{figure}

Después de aplicar los flood-fill, teníamos una imagen en BGR con el fondo en blanco y el resto en los colores correspondientes, así que, aplicamos \texttt{cv2.theshold} para convertir esta imagen en una binaria (ver \ref{section threshold}).\\ 
El threshold que utilizamos fue \texttt{THRESH\_BINARY\_INV} que deja el fondo negro y la mariposa en blanco, como deseábamos.

 Como una vez hecho esto el contorno era muy desigual, aplicamos \texttt{cv2.erode} otra vez sobre la imagen para que el borde se suavizara y de esta forma se eliminaran posibles zonas blancas aisladas que hubiera en el fondo (véase \textit{Figura \ref{sinconerode}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/sinconerode.jpg}
\end{center}
\caption{Arriba la máscara antes de aplicar \texttt{cv2.erode} y abajo después de aplicarlo.}
\label{sinconerode}
\end{figure}

Juntando todo obtenemos el programa \texttt{calc\_binary\_image.py} (ver \ref{prog calcBinaryImage}) que se divide en cuatro partes fundamentales:
\begin{itemize}
\item Un primer filtrado para acentuar los bordes de las mariposas.
\item Encontrar los contornos para utilizarlos porteriormente junto con \texttt{cv2.floodFill}.
\item Varios flood-fill en distintos puntos para tapar el fondo.
\item Un filtro posterior para eliminar pequeños fallos de \texttt{cv2.floodFill} y suavizar los bordes de la máscara.
\end{itemize}


\section{Algunos caminos no fructíferos}
\label{caminos no fructiferos}
ESTO NO SE DONDE PONERLO??
El primer objetivo del trabajo era encontrar un programa que funcionara de forma automática y, aunque no es un camino no fructífero si que es un objetivo que no hemos logrado. Debido a los diversos problemas de las muestras, encontramos muchas dificultades a la hora de encontrar unos datos que funcionasen correctamente con todas las imágenes y que a su vez, taparan las sombras. Por ello, en vez de lograr una solución que hiciera todo el trabajo de forma automática hemos logrado que lo haga de forma semiautomática, así al aplicar los flood-fill el usuario puede elegir puntos extra para adaptar mejor la máscara y eliminar las sombras que no se hayan tapado y también puede variar las constantes elegidas para cada método.\\
El valor de las constantes que dejamos por defecto funciona bien con casi todas las imágenes, pero si en alguna el usuario no obtiene lo que deseába o cree que se puede mejorar, puede cambiar estos valores para obtener soluciones más precisas.

Otros de los caminos que investigamos y de los que no obtuvimos ninguna solución fueron, los espacios de color y el programa \texttt{floodfillgray.py}.

\subsection{Espacios de color}
Lo que pretendíamos al investigar los distintos espacios de color era encontrar uno en el que pudiéramos obtener el valor de los colores independientemente de su luminosidad, para que de esta forma, la sombra no supusiera un problema a la hora de aislar la mariposa.

Investigamos distintos espacios como el YCrCb o el CIELab (ver \ref{section espacios color}), donde una de sus componentes es la luminosidad. En concreto, en YCrCb son luminosidad, cantidad de color azul y cantidad de color rojo, y en CIELab son luminosidad, posición entre rojo y verde y posición entre amarillo y azul.

Creamos \texttt{inkwell.py} y hicimos muchas pruebas, pero no encontramos ninguna ventaja si utilizábamos alguno de estos espacios en vez de RGB. Así que finalmete decidimos quedarnos en RGB.

\subsection{\texttt{floodfillgray.py}}
\label{section-floodfillgray}
La idea de este programa era separar el fondo utilizando el hecho de que es de color gris. Lo que hace es ir píxel a píxel comparando los valores de R, G y B. Por tanto, $\forall i,j$ (píxel) tenemos $R_{i,j}$, $G_{i,j}$ y $B_{i,j}$. Pintamos aquellos píxeles que cumplan $Max\{R_{i,j}, G_{i,j}, B_{i,j}\} - Min\{R_{i,j}, G_{i,j}, B_{i,j}\}<\varepsilon$ (son de color gris), $linf<\frac{R_{i,j}+G_{i,j}+B_{i,j}}{3}<lsup$ (descartamos los blancos y negros)(ver \ref{prog floodfillgray}).

Este programa fallaba en muchas muestras (véase \textit{Figura \ref{falloffg}}), en todas aquellas que tuvieran colores grises, por eso hubo que descartarle.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/falloffg.jpg}
\end{center}
\caption{Fallos de \texttt{floodfillgray.py} sobre algunas muestras.}
\label{falloffg}
\end{figure}

A pesar de que para aislar la mariposa no servía, el programa funcionaba muy bien y lo usamos para encontrar el color-check. 


\chapter{Comparando las imágenes}
Recordemos que el objetivo era que dada una nueva imagen el programa nos muestre las más parecidas a ésta utilizando criterios basados en el color y la forma de los ejemplares de las muestras. Por tanto, una vez que ya tenemos todas las imágenes a la misma escala y hemos encontrado la máscara de cada una de ellas puedemos empezar a realizar dichas comparaciones.

\section{Por color}
Las comparaciones por color las hemos hecho utilizando histogramas (ver \ref{section histogramas}) (véase \textit{Figura \ref{muestraconhistograma}}). Lo que hicimos fue utilizar la máscara para restringir la zona de la imagen sobre la cual se calcula el histograma. Después normalizarla (hacer que todos los valores varíen entre 0 y 1). Al haberlo normalizado se seleccionan mariposas con los mismos colores independientemente de si son más grandes o más pequeñas, ya que de momento sólo importaba el color (dos mariposas blancas serían seleccionadas como similares independientemente de la forma o tamaño. En el estudio de la forma ocurre lo contrario, por este motivo es conveniente juntar los dos tipos de análisis).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{imagenes/muestraconhistograma.jpg}
\end{center}
\caption{Muestra con histograma sobre cada capa.}
\label{muestraconhistograma}
\end{figure}

Una vez que se tienen calculados todos los histogramas empezamos a compararlos uno por uno con el de la imagen que deseamos introducir. Esto lo llevamos a cabo utilizando \texttt{cv2.compareHist} (ver \ref{section comparar hist}).\\
Para ello lo que hicimos fue dividir la imágen en tres capas (azul, verde, roja) y comparar los histogramas capa por capa. De cada comparación se obtiene un valor que va a indicar cómo de parecidas son las imágenes en esa capa, de forma que, si son lo suficientemente parecidas en las tres capas la imagen es seleccionada y si hay una capa en la que no se parece se descarta. Esto tiene que ser así porque si no diríamos que una imagen amarilla (con el máximo de rojo y de verde) es parecida a una blanca, ya que también tiene el máximo de rojo y verde y sólo se diferenciaría en la capa azul.

Para las comparaciones que se realizan con \texttt{cv2.compareHist} podemos elegir varios métodos (ver \ref{section comparar hist}). En nuestro caso, elegimos el de comparación por correlación y chi cuadrado (CV\_COMP\_CORREL y CV\_COMP\_CHISQR), el primero funciona un poco peor pero más rápido y el segundo al revés, más lento pero un poco mejor. 
\\Con el método de comparación por correlación, cuanto más próximo es el valor a 1 más parecidas son las imágenes y en el método de la chi-cuadrado, es mejor cuanto más próximo a 0.

El programa que realiza todo esto es \texttt{search\_by\_color.py} (ver \ref{prog searchByColor}) y tiene dos pasos principales:
\begin{itemize}
\item Encontrar las máscaras de cada imagen utilizando \texttt{calc\_binary\_image.py}.
\item Comparar una por una cada imagen con la que deseamos introducir utilizando los histogramas de cada una de ellas.
\end{itemize}

\section{Por forma}
Las comparaciones por forma las hicimos de varias maneras diferentes para quedarnos posteriormente con la que mejor resultados proporcionara. Básicamente, fueron con diferentes métodos a partir de la máscara o imagen binaria y a partir de los contornos.
 
\subsection{Con imagen binaria}
El primer método que empleamos se basaba en los momentos de una imagen.\\
Estos momentos se sacan a partir de un contorno o de una imagen binaria y nos proporcionan unos valores que representan distintas propiedades de la imagen. Los que utilizamos fueron los momentos de Hu y los momentos centrales normalizados que son invariantes a traslación, escalado y rotación, y a traslación y escalado respectivamente.\\
Las funciones que calculan estos momentos son \texttt{cv2.HuMoments} y \texttt{cv2.moments}.

\subsubsection{Momentos de Hu}
Como ya hemos dicho, estos momentos son invariantes a traslación, escalado y rotación y los calculamos a partir de la imagen binaria obtenida tras aplicar \texttt{calc\_binary\_image.py} a las imágenes de la base de datos.

Primero se calculan todos los momentos con \textit{cv2.moments} (véase \textit{Figura \ref{momentoshu}}) y después a partir de éstos, se calculan los momentos de Hu con \textit{cv2.HuMomets}. De aquí se obtienen seis valores que son los que se utilizan en la comparación.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/momentoshu.jpg}
\end{center}
\caption{Muestra con sus momentos de Hu y área.}
\label{momentoshu}
\end{figure}

El procedimiento que seguimos fue calcular estos seis valores $V_1,...,V_6$ de la imagen del nuevo ejemplar, y despues, calcular uno por uno los $v_1,...,v_6$ del resto.\\
 Cada vez que se calculan los momentos de una nueva imagen se comparan con los del nuevo ejemplar, de forma que sólo se selecciona la imagen si $v_i\in(V_i-eps_i,V_i+eps_i)$ $ \forall i\in\{1,..,6\}$ siendo cada $eps_i$ una constante que elige el usuario y que sirve para exigir que se parezcan mucho ($eps_i$ muy bajo) o permitir más diferencias ($eps_i$ más alto).

Las pruebas realizadas utilizando estos momentos no daban muy buenos resultados, ya que resultaban seleccionados muchos ejemplares que en realidad, mirando las imágenes y las correspondientes máscaras directamente, no se parecían.

A la vista de estos resultados decidimos probar con otros momentos: los momentos centrales normalizados.

\subsubsection{Momentos centrales normalizados}
 Éstos no son invariantes a rotación, algo que podía beneficiarnos, ya que si lo permitiésemos, podríamos obtener ``falsos positivos'', como por ejemplo, seleccionar una imagen que es igual a la que tenemos pero girada $180^{o}$ (véase \textit{Figura \ref{momhu}}). 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{imagenes/momhu.jpg}
\end{center}
\caption{Falsos positivos con momentos de Hu. Utilizando momentos de Hu seleccionaríamos estos dos ejemplares como parecidos en forma.}
\label{momhu}
\end{figure} 

El funcionamiento de este método es análogo al de los momentos de Hu.

\subsection{Con contornos}
\label{section contornos}
Para el resto de métodos usamos principalmente los contornos de la mariposa.

Hasta aquí teníamos un programa que calculaba una imagen binaria de la mariposa (\texttt{calc\_binary\_image.py}) y como no nos sirvía con esto, utilizamos esta imagen para calcular los contornos que necesitábamos.

Lo primero que hicimos fue aplicar \texttt{cv2.canny} y \texttt{cv2.findContours} a la máscara (ver \ref{section contornos}). Una vez hecho esto observamos que no todos los cotornos que salían estaban unidos, es decir, lo que necesitábamos era tener el contorno de la mariposa en un único trazado continuo y cerrado, pero en muchos casos no se cierra o incluso no ``bordea'' toda la mariposa (véase \textit{Figura \ref{contornocont}} y \textit{Figura \ref{contornodisc}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/contornocont.jpg}
\end{center}
\caption{Contorno continuo.}
\label{contornocont}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/contornodisc.jpg}
\end{center}
\caption{Contorno discontinuo.}
\label{contornodisc}
\end{figure}

Esto fue un problema porque la idea principal era utilizar el área de los contornos para compararlos con el área de la mariposa (este área no era conocido pero se puede aproximarlo al momento m00 que es el número de píxeles blancos de la imagen binaria) y de esta forma poder seleccionar el que necesitaba, pero al no tener contornos cerrados no se podía utilizar \texttt{cv2.contArea} ya que no proporcionaba datos fiables. 

Necesitábamos otra forma de poder seleccionar el contorno de la mariposa. Decidimos probar utilizando un método combinando el área del rectángulo y la circunferencia de mínimo área que contienen a cada contorno. Lo que hace este método es quedarse con el mínimo de estos, $A$. $A$ siempre es mayor que el área de la mariposa, $a$, así que, al compararlos hay que tenerlo en cuenta, de tal forma que, se queda con ese contorno si $A<2a$
%(creo que esta condición se puede quitar ya que no puede haber ningún contorno que tenga el valor de A mayor que el del A que contiene al contorno de la mariposa)
 y $A>a-a/2$, con la primera condición se descartan los que son muy grandes y con la segunda los que son pequeños.

Si utilizábamos sólo este método podría darse el caso de que nos quedáramos con un contorno que no estaba cerrado y existiera uno que si lo estuviera, y como si hay uno cerrado lo preferíamos siempre ante uno abierto (ya que cerrado nos proporciona más información), utilizamos conjuntamente el método del área del contorno y el del área del rectángulo y la circunferencia para seleccionar el contorno de la mariposa.\\
De cada contorno se calcula su área, si éste se parece al de la mariposa (es decir, es igual al de la mariposa +/- una cte que elige el usuario) se queda con éste y no se mira más, si no, se compara por el método del rectángulo y la circunferencia. Si por éste método es seleccionado, se guarda y se pasa al siguiente. Si al final no se ha encontrado ninguno cerrado se queda con el del método del rectángulo y la circunferencia.

En resumen, los pasos que seguimos para calcular el contorno fueron:
\begin{itemize}
\item Calcular una máscara utilizando el programa \texttt{calc\_binary\_image.py}
\item Encontrar los contornos utilizando \texttt{cv2.Canny} y \texttt{cv2.findContours}
\item Aplicar el método del rectángulo y circunferencia de mínimo área a cada contorno y probar con \texttt{cv2.contArea}. Si el segundo da un resultado correcto nos quedamos con éste, si no nos quedamos con el otro.
\end{itemize}

Esta forma de encontrar cual es el contorno de la mariposa funcionaba bastante bien, así que es el que finalmente utilizamos.

Una vez que teníamos los contornos de cada mariposa había que empezar a compararlos, en este caso, lo hicimos de dos formas diferentes.

\subsubsection{matchShapes}
El primer método que utilizamos fue basándonos en la función \texttt{cv2.matchShapes} (ver \ref{comparar contornos}). Lo que hace esta función es comparar dos contornos, es decir, dados dos contornos y la norma que queremos que utilice, devuelve un valor que representa cómo de parecidos son. Cuanto más proximo a 0 sea el valor más parecidos son los contornos.

En el programa de comparación por la forma lo que se hace es calcular este valor con tres normas diferentes obteniendo así $v_1$, $v_2$, $v_3$ y compararlo con tres constantes $c_1$,$c_2$ y $c_3$ dadas por el usuario. Se considera que los contornos son suficientemente parecidos y se selecciona la imagen cuando $v_i<c_i \forall i\in\{1,2,3\}$.

\subsubsection{Otros momentos}
El otro método que probamos se basaba en las distintas propiedades de los contornos y de la máscara de las mariposas (para más información se puede ver \url{http://www.cis.hut.fi/research/IA/paper/publications/bmvc97/bmvc97.html}). Se trataba de crear una lista con nuestros ``propios'' momentos y utilizarla para hacer la comparación igual que usábamos los tres valores de \textit{cv2.matchShapes}.

Las propiedades que se utilizan son: 
\begin{enumerate}
\item El número de píxeles en blanco o momento m00.
\item El área y el perímetro del contorno.
\item La proporción entre el ancho y el alto del rectángulo de mínimo área que contiene al contorno.
\item La proporción entre el área del contorno y el área del rectángulo de mínimo área que lo contiene.
\item La proporción entre el área del contorno y el área de la figura convexa de míninimo área que lo contiene.
\item El diámetro del círculo cuyo área es el mismo que el del contorno. Éste diámetro es $\sqrt{4*A/\pi}$ siendo A el área del contorno.
\end{enumerate}
Estos valores son muy fáciles de obtener usando las funciones \texttt{cv2.contourArea}, \texttt{cv2.arcLength}, \texttt{cv2.boundingRect} y \texttt{cv2.convexHull} que calculan el área y perímetro del contorno y el rectácgulo de mínimo área y figura convexa de mínimo área que lo contienen respectivamente.

Éstos momentos eran muy fáciles de utilizar si tuviéramos un contorno cerrado y continuo para todas las imágenes. Como no hemos conseguido obtener el contorno de dicha manera resultaba muy difícil o casi imposible calcular algunos de ellos, ya que, por ejemplo, \texttt{cv2.contourArea} no nos daba el área real del contorno por no estar cerrado.

.....REVISAR ESTA PARTE....\\
Para poder utilizar este método de forma más fiable tenemos que seguir investigando la forma de obtener un contorno continuo y cerrado para todas las imágenes.

 Como de momento no lo hemos conseguido decidimos crear \texttt{search\_by\_size\_contour.py} que comparaba las imágenes por forma utilizando \texttt{cv2.matchShapes} y los momentos 1, 3 y 6 (en 3 en vez de tomar el área del contorno que devuelve la función \texttt{cv2.contourArea} se utiliza el momento m00).

\chapter{Aplicación final}
Al llegar aquí ya teníamos creados los programas que seleccionan, dentro de una colección de imágenes de mariposas, las más parecidas a otra dada fjándonos en el color y la forma. Ahora había que juntar los dos métodos para que la criba fuera lo más precisa posible.

Al tener ya creados \texttt{search\_by\_color.py} y \texttt{search\_by\_size.py} hacer esto fue muy sencillo. Creamos \texttt{search.py} (ver \ref{prog search}) que lo que hacía es filtrar primero una colección de imágenes por forma y las que son seleccionadas las filtra, posteriormente, por color o viceversa (según el orden que elija el usuario). Obteniendo de esta manera, al final del proceso, el conjunto de imágenes deseado.

\section*{Interfaz}
El interfaz en la aplicación final tiene varias partes. Al principio, cuando se busca la máscara, hay una pantalla de configuración donde se pueden modificar valores como los límites de \texttt{cv2.Canny} o \texttt{cv2.floodFill}, aplicar nuevos flood-fill sobre la imagen para adaptarla mejor o cambiar el número de iteraciones de \texttt{cv2.medianBlur} y \texttt{cv2.erode}. También se puede elegir las ventanas que se desean tener visibles. Esta parte es común a los dos métodos de búsqueda (véase \textit{Figura \ref{config}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/config.jpg}
\end{center}
\caption{Pantalla de configuración para encontrar la máscara, pantalla de configuración en la búsqueda por color y pantalla de configuración en la búsqueda por forma.}
\label{config}
\end{figure}

Una vez que se tienen calculadas las máscaras aparece otra pantalla de configuración nueva. Ésta sí que varía dependiendo de si el método de comparación es el de color o el de forma.

Si la comparación es por color en la pantalla podemos elegir el método (correlación o chi-cuadrado) y los distintos epsilon (véase \textit{Figura \ref{config}}). 

Si la comparación es por forma las opciones exactas que aparecen depende del método que se haya elegido finalmente (momentos de Hu, momentos centrales normalizados, \texttt{cv2.matchShapes},...) pero básicamente son los distintos epsilon que se permiten de diferencia (véase \textit{Figura \ref{config}}).

En las dos comparaciones también se puede elegir ver todas las imágenes y máscaras o sólo las seleccionadas y volver a hacer la búsqueda con nuevos criterios.
   
\chapter{Conclusiones}

%hablar de lo del fondo y las sombras. De lo de los contornos discontinuos. De la dificultad hasta encontrar la máscara.De seguir investigando hasta encontrar un contorno continuo.
Unas de las conclusiones de aspecto más técnico que se sacan del trabajo tienen que ver con las muestras. Debido a los problemas que han dado las sombras en las imágenes podemos decir que sin ellas hubiera sido más sencillo encontrar una solución adecuada y que, posiblemente, los resultados serían más precisos. Para evitar la aparición de sombras se podrían tomar las imágenes con el flash en posición totalmente perpendicular al ejemplar. Como esto resulta muy complicado otra posible solución podría ser poner un fondo de un tono muy llamativo (en vez de gris o blanco) y que sea poco probable que aparezca en el ejemplar (podría ser un tono fucsia o verde (0,255,0)).

También hay que hablar de la dificultad para encontrar una máscara. Era algo aparentemente muy sencillo pero que una vez que nos pusimos a trabajar en ello nos causó muchos problemas. Posiblemente, con cambiar el tono de fondo de la imagen para evitar las sombras y conociendo el valor de este color hubiera sido una tarea bastante sencilla.

Otra de las tareas que causó bastantes problemas fue la de encontrar un contorno continuo. Esta es una de las líneas de investigación que quedan abiertas. Algunas de las posibles soluciones podrían ser crear un contorno uniendo las partes que se obtienen o unir el comienzo de una nueva línea con el final de la anterior para que así sea continuo.\\
 Pensamos que si hubiéramos logrado esto, el programa de búsqueda que clasifica los ejemplares fijándose en algunas características de la forma (sobre todo el que utiliza \texttt{cv2.matchShapes} y el de otros momentos, ver \ref{section contornos}) sería más eficaz y proporcionaría mejores resultados.

En general hay que decir que el trabajo no siempre a sido fácil y a lo largo del documento se muestra un poco de todo. Hemos encontrado dificultades, algunos caminos que dieron muchos quebraderos de cabeza y que al final no solucionaban nada, otros caminos que también los dieron pero acabaron dando fruto y momentos que parecía que no iban a llegar nunca, en los que por fin, se encuentran algunas soluciones. Además hay que decir que como en todo viaje no todo ha sido difícil, también ha habido situaciones en que parecía que las cosas salían solas y eran momentos muy reconfortantes que te animaban a continuar.



\appendix
\chapter{Conceptos generales del tratamiento de imágenes}
\label{opencv}
OpenCV (Open source Computer Vision library) es una librería de Visión Computarizada, destinada al tratamiento de imágenes y, principalmente, a la visión por computador en tiempo real. Contiene gran cantidad de funciones que abarcan una gran gama de áreas en el proceso de visión, como reconocimiento de objetos, calibración de cámaras o visión robótica. Se puede utilizar con varios lenguajes de programación C, C++ o, como en nuestro caso, Python.

Detrás de la mayoría de las funciones de la librería que vamos a utilizar se encuentran gran cantidad de cálculos, fórmulas y, en definitiva, métodos matemáticos. Por ello, en este apéndice, se explican conceptos generales del tratamiento de imágenes utilizando las principales funciones de OpenCV que van a ser necesarias para desarrollar el proyecto, centrándonos principalmente en entender su funcionamiento y tratar de explicar algo sobre las matemáticas que hay detrás.

Antes de empezar, es conveniente explicar que un ordenador ``ve'' una imagen como una matriz, donde en cada posición hay tres números, el valor del color rojo, verde y azul (cada elemento de la matriz representa un píxel). Otra forma de verlo es como si la imagen se separara en tres matrices o ``capas'', una la de los valores del color rojo, otra verde y otra azul.\\
En imágenes en escala de grises sólo habría una matriz donde cada número representa el valor del gris en ese píxel (véase \textit{Figura \ref{matriz}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/matriz.jpg}
\end{center}
\caption{El ordenador ve la imagen como una matriz de números. Si la imagen fuera a color en vez de una matriz serían tres.}
\label{matriz}
\end{figure}

El trabajo lo he desarrollado utilizando Python 2.7 y OpenCV 2.3.1.

\section{Filtrado de imágenes}
Tenemos una imagen y queremos aplicar sobre ella un filtro. Los filtros que vamos a usar lo que hacen es, dada la imagen y un núcleo, se desplaza píxel por píxel sobre la imagen comparando los valores de la posición $(i,j)$ con todos los del núcleo y obteniendo uno nuevo. El valor que tenía el píxel $(i,j)$ se cambia por el nuevo. Dependiendo del método que utilicemos el núcleo tendrá tamaños diferentes pudiendo llegar a ser de tamaño $1\times1$ y el método de comparación también será distinto.

\subsection {Smoothing}
\label{section blur}
Estos filtros se utilizan frecuentemente y se usan principalmente para eliminar ruido de las imágenes o reducir la resolución. Hay muchos tipos pero ahora vamos a centrarnos sólo en uno, ya que es el que más utilizamos a lo largo del documento, median blur.

En el programa \texttt{smoothing.py} se pueden ver ejemplos con varios métodos diferentes.

\subsubsection {Median blur}
En este caso, no hay que dar un núcleo concreto, sólo hay que dar el tamaño deseado, supongamos $k$ con $k\ge 1$ impar. El núcleo será igual a la porción de imagen de tamaño $k\times k$ coincidiendo el centro con el píxel $(i,j)$. Lo que hace es ir recorriendo toda la imagen de forma que en la posición $(i,j)$, $a_{i,j}$ ahora será igual a la media de todos los valores del núcleo, es decir, $ a_{i,j} = \displaystyle\sum_{n = i-\frac{k-1}{2}}^{i+\frac{k-1}{2}}\displaystyle\sum_{m = j-\frac{k-1}{2}}^{j+\frac{k-1}{2}} \frac{b_{n,m}}{k^2}$.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/normalmb.jpg}
\end{center}
\caption{Imagen normal arriba e imagen tras aplicar el filtro median blur abajo.}
\label{medianblur}
\end{figure}

\subsection {Erode y dilate}
\label{section erode dilate}
Los filtros erode y dilate se utilizan para eliminar ruido, aislar y juntar elementos, etc. También pueden utilizarse para encontrar contornos en una imagen.

Dado el tamaño del núcleo y el punto de éste donde queremos centrar cada píxel (por defecto es el centro del núcleo), se recorre toda la imagen calculando para cada posición un nuevo valor que varía dependiendo de si es erode o dilate:
\begin{itemize}
\item{\bfseries{Erode:}}
\\$a_{i,j} = Min_{n,m}\{a_{n,m}\}$ con $n\in [i-\frac{k-1}{2},i+\frac{k-1}{2}]$ y $m\in [j-\frac{k-1}{2},j+\frac{k-1}{2}]$

Al tomar el valor mínimo lo que se logra es oscurecer la imagen o agrandar las zonas oscuras. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/erode.jpg}
\end{center}
\caption{Imagen tras aplicar el método  \texttt{cv2.erode}.}
\label{config2forma}
\end{figure}

\item{\bfseries{Dilate:}}
\\$a_{i,j} = Max_{n,m}\{a_{n,m}\}$ con $n\in [i-\frac{k-1}{2},i+\frac{k-1}{2}]$ y $m\in [j-\frac{k-1}{2},j+\frac{k-1}{2}]$

Al tomar el valor máximo lo que se logra es aclarar la imagen o agrandar las zonas claras.

\end{itemize}
 
\subsection {Threshold}
\label{section theshold}
Estos filtros tienen muchas aplicaciones en el procesamiento de imágenes y además su funcionamiento es muy sencillo.\\
Dada una imagen y un límite $L$ se recorre toda la imagen comparando cada píxel con $L$ y tomando una decisión sobre ese píxel dependiendo del tipo de threshold que hayamos elegido. 

Hay muchos tipos diferentes de thresholds: \texttt{THRESH\_BINARY}, \texttt{THRESH\_TRUNC}, \texttt{THRESH\_TOZERO}, etc. (véase \textit{Figura \ref{thresh}}) y algunos un poco más complejos como \texttt{cv2.adaptiveThreshold} que funciona muy bien cuando hay mucha luz o cambios de iluminación dentro de la misma imagen. En este último lo que cambia principalmente es que en vez de fijarnos únicamente en un pixel para hacer la comparación se utilizan otros métodos, como por ejemplo la media. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/thresh.jpg}
\end{center}
\caption{Diferentes tipos de thresholds.}
\label{thresh}
\end{figure}


\subsection{Floodfill}
\label{section floodfill}
El método Flood Fill es muy útil y se utiliza sobre todo para aislar o resaltar partes de una imágen para un análisis posterior. Sirve para encontrar máscaras o reducir el procesamiento a una zona en concreto haciendo que sea más rápido.

Lo que hace esta función es dada una imágen, un punto, un límite inferior, uno superior y un nuevo valor $(r,g,b)$, cambia al nuevo $(r,g,b)$ los píxeles cuyos valores estén entre el límite inferior y el superior y que además estén conectados a otro que cumpla lo mismo. Es decir, encontramos una componente conexa de la imagen a partir del punto, que cumple que todos sus píxel están entre dos valores dados (véase \textit{Figura \ref{ffnormal}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/floodfillnormal.jpg}
\end{center}
\caption{Ejemplo de aplicación del método \texttt{cv2.floodFill}.}
\label{ffnormal}
\end{figure}

El método se puede utilizar con o sin máscara. Si lo utilizamos con máscara lo que logramos es que no atraviese aquellos píxeles que tienen un valor distinto de cero en la máscara.

Se puede ver el funcionamiento del método y ejemplos en los programas\\ \texttt{analisys/imgfloodfill.py} y \texttt{analisys/floodfill\_img\_mask.py}.
 
\section{Análisis de imágenes}
Hay muchas y muy variadas funciones y métodos que se utilizan para el análisis de imágenes, pero aquí nos vamos a centrar en los relacionados con el color y en concreto en los histogramas y espacios de color.

\subsection{Histogramas}
\label{section histogramas} 
Los histogramas son una herramienta muy utilizada en Computer Vision debido a su gran variedad de aplicaciones como por ejemplo, detectar cambios de escena, puntos de interés o el reconocimiento de objetos.

Como ya hemos dicho un histograma puede tener muchos usos pero aquí vamos a utilizarlo para contabilizar la cantidad de píxeles de cada color que tiene una imagen.\\
Dada la imagen y el tamaño del histograma (en este caso 256) se va a crear un array en el cual la posición 0 va a representar la cantidad de píxeles con valor 0 que hay en la imagen, la 1, la cantidad de píxeles con valor 1, y así sucesivamente hasta el 255. Estos datos se pueden representar en forma de gráfico (véase \textit{Figura \ref{hist}}).

\begin{figure}[ht]
\begin{center}
\includegraphics[width=6cm]{imagenes/histograma.jpg}
\end{center}
\caption{Histograma.}
\label{hist}
\end{figure}

También se puede utilizar una máscara para calcular el histograma únicamente sobre los píxeles de la máscara con valores distintos de cero.
\subsubsection{Comparar histogramas}
\label {section comparar hist}
Una vez que tenemos calculados los histogramas una de las operaciones que podemos hacer con ellos es compararlos.

Para hacer esta comparación se pueden utilizar varios métodos distintos dependiendo de la métrica que utilicemos (correlación, chi-cuadrado, intersección, Bhattacharyya).\\
En este documento las métricas que hemos utilizado son la de correlación y la chi-cuadrado.
\begin{itemize}
\item {\bfseries{Correlación:}}
$d(H_1,H_2) = \frac{\displaystyle\sum_{i} H'_1(i)H'_2(i)}{\sqrt{\displaystyle\sum_{i} H'^2_1(i)H'^2_2(i)}}$ con $H'_k(i) = H_k(i)-(1/N)(\displaystyle\sum_{j} H_k(j))$ siendo $N$ el tamaño del histograma.\\
Un ajuste perfecto daría como resultado 1 y cuanto peor sea más cercano de -1.
\item{\bfseries{Chi-cuadrado:}}
$d(H_1,H_2) = \displaystyle\sum_{i} \frac{(H_1(i)-H_2(i))^2}{H_1(i)+H_2(i)}$\\
Un ajuste perfecto daría como resultado 0 y cuanto peor sea el ajuste más grande será el resultado (este valor no está acotado).
\end{itemize}

\subsection{Espacios de color}
\label{section espacios color}
Los espacios de color son modelos matemáticos abstractos que describen formas de representar los colores mediante números (generalmente 3). Hay muchos modelos diferentes, entre ellos: LUV, YCrCb, CieXYZ, HSV, CieLab y el más conocido, RGB.

En este caso vamos a estudiar más a fondo los tres últimos.

\begin{itemize}
\item{\bfseries{HSV}}
\\En este espacio cada color viene dado por tres números, el primero representa la H (\textit{hue}, tono o matiz), el segundo la S (\textit{saturation}, saturación) y el tercero la V (\textit{value}, valor).

El tono se da en grados y varía entre $0^o$ y $360^o$.\\
La saturación indica la distancia al eje de brillo blanco-negro y varía entre $0^o$ y $100^o$.\\
El valor indica la altura en el eje blanco-negro y varía entre $0^o$ y $100^o$, siendo 0 negro.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=6cm]{imagenes/hsv.jpg}
\end{center}
\caption{Modelo espacio de color HSV.}
\label{hsv}
\end{figure}

\item{\bfseries{CieLab}}
\\En este espacio el primer valor representa la L (luminosidad), el segundo la $a$ (posición entre rojo y verde) y el tercero la $b$ (posición entre amarillo y azul).

La luminosidad varía entre 0 y 100 siendo 0 negro y 100 blanco.\\
La $a$ varía entre -128 y 127, valores negativos indican verde y positivos rojo.\\
la $b$ varía entre -128 y 127, valores negativos indican azul y positivos amarillo.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=6cm]{imagenes/cielab.jpg}
\end{center}
\caption{Modelo espacio de color CieLab.}
\label{lab}
\end{figure}

\item{\bfseries{RGB}}
\\En el espacio de color RGB la R (\textit{red}) representa la cantidad de color rojo, la G (\textit{green}) la cantidad de color verde y la B (\textit{blue}) la cantidad de color azul. Los tres valores se mueven entre 0 y 255, siendo (0,0,0) negro y (255,255,255) blanco.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=6cm]{imagenes/rgb.jpg}
\end{center}
\caption{Modelo espacio de color RGB.}
\label{rgb}
\end{figure}

\end{itemize}

\section{Contornos}
\label{section contornos}
Muchos de los algoritmos que se utilizan para la detección de bordes están basados en el cálculo del gradiente, es decir, la dirección de máxima variación (cuánto más grande sea el gradiente más probabilidad hay de que sea un elemento del contorno). Algunos de estos métodos son \texttt{cv2.Sobel}, \texttt{cv2.Laplacian} o \texttt{cv2.Canny}. Después se puede aplicar \texttt{cv2.findContours} sobre una de las imágenes conseguidas con éstos métodos para obtener los contornos de diferntes formas (sólo los exteriores, en forma de árbol, etc.).

En esta sección se explica con más detalle la función Canny ya que es la más utilizado a lo largo del proyecto.

Una vez que se tienen calculados los contornos se pueden utilizar para muchas aplicaciones, combinándolos con otros métodos, como máscaras o por sí solos. Utilizando por ejemplo las funciones \texttt{cv2.contourArea} o \texttt{cv2.arcLength} es muy fácil calcular el área y el perímetro respenctivamente de los contornos. Aquí nos vamos a centrar en explicar como comparar dos contornos.

\subsection{Canny}
\label{section canny}
Lo que hace esta función es aplicar el algoritmo Canny con los límites superior e inferior dados por el usuario. Es decir, primero aplica un filtro gaussiano para reducir el ruido y despues busca los bordes en cuatro direcciones: vertical, horizontal y en las diagonales. Una vez hecho ésto, va fíjandose píxel por píxel. Si el píxel tiene un gradiente mayor que el límite superior se toma como elemento del borde, si está por debajo del límite inferior se descarta y si está entre los dos límites se toma como borde solo sí está conectado a otro píxel que está por encima del límite superior. 

\subsection{Comparar contornos}
\label{comparar contornos}
Cuando ya se tienen calculados los contornos una de las operaciones que se pueden llevar a cabo con ellos es compararlos. Para ello se puede utilizar la función \texttt{cv2.matchShapes}.

Esta función lo que hace es dados dos contornos y una métrica calcula un valor (utilizando los momentos invariantes de Hu) que indica cómo de parecidos son. Las métricas son las de la \textit{Fig \ref{matchshapes}}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/matchShapes.jpg}
\end{center}
\caption{Posibles métricas en la función  \texttt{cv2.matchShapes}.}
\label{matchshapes}
\end{figure}

Además de comparar dos contornos con la función \texttt{cv2.matchShapes} se pueden utilizar otras propiedades de los contornos muy sencillas de calcular como por ejemplo, el área, el perímetro, la proporción entre base y altura del rectángulo de mínimo área que lo contiene, etc. (\url{http://www.cis.hut.fi/research/IA/paper/publications/bmvc97/bmvc97.html})
 
\chapter{Código}
\section{resize\_and\_write.py}
\label{prog resizeAdWrite}
{\tiny
\lstinputlisting{../resize_and_write.py}
}
\section{floodfillgray.py}
\label{prog floodfillgray}
{\tiny
\lstinputlisting{../floodfillgray.py}
}
\section{Calc\_binary\_image.py}
\label{prog calcBnaryImage}
{\tiny
\lstinputlisting{../calc_binary_image.py}
}
\section{search\_by\_color.py}
\label{prog searchByColor}
{\tiny
\lstinputlisting{../search_by_color.py}
}
\section{search.py}
\label{prog search}
{\tiny
\lstinputlisting{../search.py}
}

\chapter{Pruebas}
Para realizar las pruebas que se indican en el documento no se han usado todas las imágenes de la base de datos, solo una pequña colección. La imagen se desea introducir y con la que se comparan es la imagen principal, que está incluida dentro de esta colección.

Además de las que aparecen plasmadas en este documento se han realizado más pruebas utilizando todas las imágenes de la base de datos y con cada uno de los métodos que han sido implemtados. Algunas pruebas daban buenos resultados y otras un poco peores y las que aquí se encuentran pretenden enseñar una pequeña muestra de cada tipo.

En general, tras realizar las pruebas, se observa que el método de comparación por color funciona bastante bien y el de forma es menos fiable, ya que proporciona peores  soluciones.
\section{Pruebas de \texttt{search\_by\_color.py}}
Para la prueba del programa \texttt{search\_by\_color.py} se han tomado las imáganes que se muestran en la figura \ref{todascolor} de la base de datos y éstas se comparan con el ejemplar de la figura \ref{principal1}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/todascolor.jpg}
\end{center}
\caption{Colección que vamos a usar para las comparaciones.}
\label{todascolor}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/principal1.jpg}
\end{center}
\caption{Imagen que se desea introducir.}
\label{principal1}
\end{figure}

El método que se ha usado en este caso para la comparación es el de correlación. Con parámetros (17,15,15) se seleccionan 4 candidatas, las de la figura \ref{solucion1}.
 
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=15cm]{imagenes/pruebas/solucion1.jpg}
\end{center}
\caption{Muestras seleccionadas tras aplicar el método.}
\label{solucion1}
\end{figure}

Se observa que cuando empezamos a disminuir los valores de epsilon (a permitir que se parezcan menos) se empiezan a seleccionar más, entre ellas, la primera nueva que aparece es la de la figura \ref{nueva1}. Si por el contrario, se aumentan un poco estos valores, a (17,17,17), empiezan a seleccionarse cada vez menos muestras. 

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=10cm]{imagenes/pruebas/nueva1.png}
\end{center}
\caption{Nueva muestra seleccionada al variar las constantes.}
\label{nueva1}
\end{figure}

\section{Pruebas de \texttt{search\_by\_size.py}}
Como las comparaciones por forma las hicimos de varias maneras diferentes vamos a hacer puebras con dos de esos métodos.

\subsection*{Prueba con momentos centrales normalizados}
Para esta prueba se utiliza el programa \texttt{search\_by\_size\_nu.py} con las constantes 100, 50, 20, 150, 150, 150, 250. Las imágenes que utilizamos de la base de datos son las de la figura \ref{todasforma1} y la imagen que se desearía introducir y con la que se van a comparar todas las demás es la de la figura \ref{principal2}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/todasforma1.jpg}
\end{center}
\caption{Colección que vamos a usar para las comparaciones.}
\label{todasforma1}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/principal2.jpg}
\end{center}
\caption{Imagen que se desea introducir.}
\label{principal2}
\end{figure}

Se observa que permitiendo una diferencia de área de como máximo 250 píxeles, se seleccionan 2 imágenes, las de la figura \ref{solucion2}. Si miramos las imágenes a simple  vista quizá se podria decir que faltan algunas que deberían estar (véase figura \ref{faltan}), pero si nos fijamos en el tamaño realmente no es así, ya que son de diferente tamaño al de la muestra (unas más grandes y otras más pequeñas, fijandose en la regla se observa bien).

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/faltan.jpg}
\end{center}
\caption{Muestras que a simple vista parece que faltan.}
\label{faltan}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/solucion2.jpg}
\end{center}
\caption{Muestras que resultan seleccionadas.}
\label{solucion2}
\end{figure}

\subsection*{Prueba con contornos}
Para esta prueba se utiliza el programa \texttt{search\_by\_size\_contour.py} con las constantes predeterminadas. Las muestras que tomamos de la base de datos son las de la figura \ref{todasforma2} y la que se desea introducir en \ref{principal3}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/todasforma2.jpg}
\end{center}
\caption{Colección que vamos a usar para las comparaciones.}
\label{todasforma2}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/principal3.jpg}
\end{center}
\caption{Imagen que se desea introducir.}
\label{principal3}
\end{figure}

En este caso se seleccionan 3 imágenes (véase figura \ref{solucion3}), si nos fijamos en las imágenes a simple vista parece que faltan 2 (véase figura \ref{faltan2}) pero si modificamos las constantes para permitir más diferencias se empiezan a detectar como parecidas algunas que claramente no lo son como figura \ref{nosolucion}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/solucion3.jpg}
\end{center}
\caption{Muestras que resultan seleccionadas.}
\label{solucion3}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/pruebas/faltan2.jpg}
\end{center}
\caption{Muestras que a simple vista parece que faltan.}
\label{faltan2}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/nosolucion.jpg}
\end{center}
\caption{Muestra que se selecciona al variar las constantes y que no debería.}
\label{nosolucion}
\end{figure}

Para ver si realmente las muestras que parece que faltan son un falso negativo o el problema es por pérdida de información al calcular la máscara podemos observar las máscaras de todas (ver figura \ref{todasmascaras}) y de las seleccionadas (ver \ref{mascarasseleccionadas}).

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/pruebas/todasmascaras.jpg}
\end{center}
\caption{Máscaras de cada muestra que estamos comparando.}
\label{todasmascaras}
\end{figure}
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=12cm]{imagenes/pruebas/mascarasseleccionadas.jpg}
\end{center}
\caption{Máscaras de las muestras que han resultado seleccionadas.}
\label{mascarasseleccionadas}
\end{figure}

Las imágenes que parece que faltan a simple vista son la resizedBMC-1933\_D.jpg y resizedBMC-1936\_D.jpg y la que se desea introducir es resizedBMC-1905\_D.jpg. Fijándonos en las máscaras de estas muestras y comparándolas con la principal se observa que sí que son parecidas y que, aunque cambiando los valores de las constantes, deberían llegar a aparecer.

\section{Pruebas de \texttt{search.py}}
Para la prueba por color se ha utilizado en la comparación por forma el programa \texttt{search\_by\_size\_contour.py}. En este caso se ha hecho primero selección por color utilizando correlación con valores 14, 14, 14 y posteriormente, comparación por forma con los valores predeterminados sobre las muestras que ya habían sido seleccionadas.

Las muestras que se han utilizado son las de la figura \ref{todas} y la imagen principal es la de la figura \ref{principal4}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/todas.jpg}
\end{center}
\caption{Colección que vamos a usar para las comparaciones.}
\label{todas}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/principal4.jpg}
\end{center}
\caption{Imagen que se desea introducir.}
\label{principal4}
\end{figure}

Salen seleccionadas en la primera fase por color las muestras de la figura \ref{seleccioncolor} y a simple vista no parece que haya ningún falso positivo ni negativo.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=16cm]{imagenes/pruebas/seleccioncolor.jpg}
\end{center}
\caption{Muestras que son seleccionadas en la fase de búsqueda por color.}
\label{seleccioncolor}
\end{figure}

En la segunda fase se selecciona una única imagen, la de la figura \ref{solucionforma} que es ella mmisma. Como antes, a simple vista el funcionamiento a sido correcto.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=8cm]{imagenes/pruebas/principal4.jpg}
\end{center}
\caption{Muestra seleccionada tras aplicar las dos fases del programa.}
\label{solucionforma}
\end{figure}

\end{document}
